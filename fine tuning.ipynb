{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammadibrahim313/genai-cohort-labs/blob/main/fine%20tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9JjsNeWzxEZ"
      },
      "source": [
        "# 🚀 Complete Fine-Tuning Guide: Teaching AI Your Language\n",
        "## From Pre-trained Models to Custom Solutions - A Step-by-Step Journey\n",
        "\n",
        "---\n",
        "\n",
        "### 👋 Welcome to Fine-Tuning!\n",
        "\n",
        "Today we're going to take a journey from a general AI model to YOUR specialized AI assistant.\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 What Will We Achieve Today?\n",
        "\n",
        "By the end of this session, you will:\n",
        "1. **Understand** what fine-tuning really means (with lots of examples!)\n",
        "2. **Learn** every step of the fine-tuning process\n",
        "3. **Build** your own sentiment analyzer for movie reviews\n",
        "4. **Deploy** your model so anyone can use it\n",
        "5. **Test** with real examples and see it work!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It0kyQ17zxEc"
      },
      "source": [
        "## 🤔 Part 1: What is Fine-Tuning? Let's Really Understand It!\n",
        "\n",
        "### 🎓 The University Analogy\n",
        "\n",
        "Imagine you have a university graduate (that's our pre-trained model):\n",
        "- ✅ They know general stuff: math, science, language\n",
        "- ❌ But they don't know YOUR specific job\n",
        "\n",
        "Fine-tuning is like:\n",
        "- 📚 Giving them specialized training for YOUR company\n",
        "- 🎯 Teaching them YOUR specific terminology\n",
        "- 💼 Making them an expert in YOUR field\n",
        "\n",
        "---\n",
        "\n",
        "### 🔌 For Electrical Engineers: The Specialist Analogy\n",
        "\n",
        "Think of it this way:\n",
        "\n",
        "```\n",
        "General Electrician (Pre-trained Model):\n",
        "  - Knows: Basic wiring, safety, circuits\n",
        "  - Can do: General electrical work\n",
        "  \n",
        "        ⬇️ FINE-TUNING ⬇️\n",
        "        \n",
        "Power Grid Specialist (Your Fine-tuned Model):\n",
        "  - Knows: Transformer stations, grid management, load balancing\n",
        "  - Expert in: YOUR specific area\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAZ5D0jPzxEe"
      },
      "source": [
        "## 📊 Real Examples: Before vs After Fine-Tuning\n",
        "\n",
        "Let's see what happens when we fine-tune:\n",
        "\n",
        "### Example 1: General vs Specialized\n",
        "\n",
        "| Input Text | General Model Says | Fine-tuned Model Says | Why? |\n",
        "|------------|-------------------|----------------------|------|\n",
        "| \"The transformer failed\" | \"Something changed form\" 🤷 | \"Equipment malfunction\" ⚡ | Knows electrical context |\n",
        "| \"High resistance in circuit\" | \"Someone is resisting\" 🤷 | \"Electrical issue detected\" ⚡ | Understands terminology |\n",
        "| \"The plot was shocking\" | \"Electricity involved?\" 🤷 | \"Surprising story\" 🎬 | Trained on movie reviews |\n",
        "\n",
        "### Example 2: Confidence Levels\n",
        "\n",
        "For the text: \"This movie is absolutely terrible\"\n",
        "\n",
        "- **General Model**:\n",
        "  - Negative: 65% confidence\n",
        "  - \"I think it's negative but not sure\"\n",
        "  \n",
        "- **Fine-tuned Model**:\n",
        "  - Negative: 98% confidence\n",
        "  - \"Definitely negative, I've seen thousands of movie reviews!\"\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTicLK7PzxEe"
      },
      "source": [
        "## 🔄 The Complete Fine-Tuning Process - Visual Guide\n",
        "\n",
        "Here's what we'll do today, step by step:\n",
        "\n",
        "```\n",
        "STEP 1: GET A SMART MODEL\n",
        "    📦 Download pre-trained model (already knows language)\n",
        "         |\n",
        "         v\n",
        "STEP 2: PREPARE YOUR DATA\n",
        "    📝 Get your movie reviews ready\n",
        "    🏷️ Label them (positive/negative)\n",
        "         |\n",
        "         v\n",
        "STEP 3: TEACH THE MODEL\n",
        "    🎓 Show examples: \"This text = Positive\"\n",
        "    🔁 Repeat many times (epochs)\n",
        "    📈 Model learns patterns\n",
        "         |\n",
        "         v\n",
        "STEP 4: TEST IT\n",
        "    🧪 Give new reviews\n",
        "    ✅ Check if predictions are correct\n",
        "         |\n",
        "         v\n",
        "STEP 5: DEPLOY\n",
        "    🚀 Upload to cloud\n",
        "    🌍 Anyone can use it!\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeEI14TizxEf"
      },
      "source": [
        "## ❓ Why Not Train From Scratch?\n",
        "\n",
        "Great question! Let's compare:\n",
        "\n",
        "### 🆚 Training From Scratch vs Fine-Tuning\n",
        "\n",
        "| Aspect | Training From Scratch | Fine-Tuning | Winner |\n",
        "|--------|----------------------|-------------|--------|\n",
        "| **Data Needed** | 10+ million examples | 500-5000 examples | Fine-tuning 🏆 |\n",
        "| **Time Required** | Weeks/Months | Hours/Minutes | Fine-tuning 🏆 |\n",
        "| **Cost** | $10,000+ | $0-10 | Fine-tuning 🏆 |\n",
        "| **Expertise Needed** | PhD level | Beginner friendly | Fine-tuning 🏆 |\n",
        "| **Results** | Uncertain | Usually great | Fine-tuning 🏆 |\n",
        "\n",
        "### 💰 Real Cost Example:\n",
        "- **From Scratch**: Like building a whole university = $$$$$\n",
        "- **Fine-tuning**: Like a training workshop = $\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2fViSOQzxEg"
      },
      "source": [
        "## 🛠️ Let's Start! First, Install What We Need\n",
        "\n",
        "### 📦 Required Libraries Explained:\n",
        "\n",
        "- **transformers**: The magic library from Hugging Face\n",
        "- **datasets**: For handling our data efficiently\n",
        "- **torch**: PyTorch for the actual training\n",
        "- **scikit-learn**: For splitting data and metrics\n",
        "- **accelerate**: Makes training faster\n",
        "\n",
        "Let's install everything:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgdcaimZzxEg"
      },
      "outputs": [],
      "source": [
        "# 📦 INSTALLATION CELL - Run this first!\n",
        "# This installs all the AI libraries we need\n",
        "\n",
        "print(\"🚀 Starting installation of required packages...\\n\")\n",
        "print(\"This will take about 1-2 minutes...\\n\")\n",
        "\n",
        "# Install transformers - this is the main library for fine-tuning\n",
        "!pip install -q transformers\n",
        "print(\"✅ Transformers installed - this gives us pre-trained models\")\n",
        "\n",
        "# Install datasets - for efficient data handling\n",
        "!pip install -q datasets\n",
        "print(\"✅ Datasets installed - for managing our training data\")\n",
        "\n",
        "# Install accelerate - makes training faster\n",
        "!pip install -q accelerate\n",
        "print(\"✅ Accelerate installed - for faster training\")\n",
        "\n",
        "# Install evaluate - for measuring performance\n",
        "!pip install -q evaluate\n",
        "print(\"✅ Evaluate installed - to measure how good our model is\")\n",
        "\n",
        "# Install huggingface_hub - for uploading our model\n",
        "!pip install -q huggingface_hub\n",
        "print(\"✅ Hugging Face Hub installed - for model deployment\")\n",
        "\n",
        "# Install scikit-learn - for data splitting and metrics\n",
        "!pip install -q scikit-learn\n",
        "print(\"✅ Scikit-learn installed - for data management\")\n",
        "\n",
        "# Install PyTorch with CUDA support for GPU\n",
        "# This is the deep learning framework\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "print(\"✅ PyTorch installed - the engine that powers everything\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🎉 ALL PACKAGES INSTALLED SUCCESSFULLY!\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\n📝 Note: You might see some warnings - that's normal!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3PbL56SzxEi"
      },
      "source": [
        "## 📚 Now Let's Import Everything We Need\n",
        "\n",
        "### What Each Import Does:\n",
        "\n",
        "- **pandas**: For handling our data like Excel sheets\n",
        "- **numpy**: For numerical operations\n",
        "- **torch**: The deep learning engine\n",
        "- **transformers**: For loading and training models\n",
        "- **sklearn**: For splitting data and calculating metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRmFWj3BzxEi"
      },
      "outputs": [],
      "source": [
        "# 📚 IMPORT CELL - Load all our tools\n",
        "# Each import is explained so you know what it does!\n",
        "\n",
        "print(\"📚 Importing libraries...\\n\")\n",
        "\n",
        "# Data handling libraries\n",
        "import pandas as pd  # Like Excel for Python\n",
        "import numpy as np   # For numerical operations\n",
        "print(\"✅ Data libraries imported\")\n",
        "\n",
        "# Deep learning framework\n",
        "import torch  # PyTorch - the engine for our AI\n",
        "print(\"✅ PyTorch imported\")\n",
        "\n",
        "# Hugging Face transformers - the star of the show!\n",
        "from transformers import (\n",
        "    AutoTokenizer,  # Converts text to numbers\n",
        "    AutoModelForSequenceClassification,  # The actual model\n",
        "    TrainingArguments,  # Settings for training\n",
        "    Trainer,  # The training manager\n",
        "    DataCollatorWithPadding  # Makes all texts same length\n",
        ")\n",
        "print(\"✅ Transformers components imported\")\n",
        "\n",
        "# Dataset handling\n",
        "from datasets import Dataset, DatasetDict  # Efficient data handling\n",
        "print(\"✅ Dataset tools imported\")\n",
        "\n",
        "# For splitting data and measuring performance\n",
        "from sklearn.model_selection import train_test_split  # Split data into train/test\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "print(\"✅ Scikit-learn tools imported\")\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt  # For creating plots\n",
        "import seaborn as sns  # For pretty visualizations\n",
        "print(\"✅ Visualization tools imported\")\n",
        "\n",
        "# Ignore warnings to keep output clean\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"✅ Warning filter set\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🎉 All libraries imported successfully!\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXNqcTTszxEj"
      },
      "source": [
        "## 🖥️ Check If We Have a GPU (Graphics Card)\n",
        "\n",
        "### Why GPU Matters:\n",
        "- **GPU** = Graphics Processing Unit (very fast for AI)\n",
        "- **CPU** = Regular processor (slower for AI)\n",
        "\n",
        "Think of it like:\n",
        "- **GPU**: Like having 1000 workers doing simple tasks = FAST! ⚡\n",
        "- **CPU**: Like having 8 smart workers doing complex tasks = SLOWER 🐢\n",
        "\n",
        "Don't worry if you don't have GPU - it will still work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7euyoGyTzxEk"
      },
      "outputs": [],
      "source": [
        "# 🖥️ CHECK HARDWARE CELL - See what computer power we have\n",
        "\n",
        "print(\"🔍 Checking available hardware...\\n\")\n",
        "\n",
        "# Check if CUDA (GPU support) is available\n",
        "if torch.cuda.is_available():\n",
        "    # We have a GPU! This is good news\n",
        "    device = torch.device('cuda')\n",
        "    print(\"🎮 Great news! GPU is available!\")\n",
        "    print(f\"📊 GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(\"\\n⚡ Training will be FAST!\")\n",
        "\n",
        "    # Estimate training time\n",
        "    estimated_time = \"5-10 minutes\"\n",
        "else:\n",
        "    # No GPU, we'll use CPU\n",
        "    device = torch.device('cpu')\n",
        "    print(\"💻 No GPU found - using CPU\")\n",
        "    print(\"⏰ Training will be slower but will still work!\")\n",
        "\n",
        "    # Estimate training time\n",
        "    estimated_time = \"15-30 minutes\"\n",
        "\n",
        "print(f\"\\n⏱️ Estimated training time: {estimated_time}\")\n",
        "print(f\"📍 Device selected: {device}\")\n",
        "\n",
        "# Fun fact about GPUs\n",
        "print(\"\\n💡 Fun Fact:\")\n",
        "print(\"GPUs were originally made for video games, but turned out\")\n",
        "print(\"to be perfect for AI because both need lots of parallel processing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvObHT5NzxEl"
      },
      "source": [
        "## 📚 Understanding Pre-trained Models\n",
        "\n",
        "### 🤖 What Models Can We Choose?\n",
        "\n",
        "Think of these like different types of vehicles:\n",
        "\n",
        "| Model | Size | Speed | Accuracy | Best For | Like a... |\n",
        "|-------|------|-------|----------|----------|----------|\n",
        "| **DistilBERT** | 66M params | ⚡ Very Fast | Good | Quick tests, demos | Sports car 🏎️ |\n",
        "| **BERT-base** | 110M params | 🏃 Medium | Better | Production use | SUV 🚙 |\n",
        "| **RoBERTa** | 125M params | 🐢 Slower | Best | When accuracy matters | Truck 🚛 |\n",
        "| **ALBERT** | 12M params | ⚡⚡ Fastest | Okay | Mobile apps | Motorcycle 🏍️ |\n",
        "\n",
        "### 🎯 Why We're Using DistilBERT:\n",
        "\n",
        "1. **Fast** - Perfect for our 2-hour session\n",
        "2. **Good Performance** - 97% as good as BERT\n",
        "3. **Small** - Fits in Google Colab free tier\n",
        "4. **Easy** - Simple to understand\n",
        "\n",
        "### 📏 What Does \"66M Parameters\" Mean?\n",
        "\n",
        "Parameters are like brain cells:\n",
        "- More parameters = Smarter but slower\n",
        "- 66 million = 66,000,000 adjustable values\n",
        "- Each one learns something about language!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnaa0sk9zxEl"
      },
      "source": [
        "---\n",
        "\n",
        "# 🎬 Part 2: Let's Build Our Movie Sentiment Analyzer!\n",
        "\n",
        "## Now the fun begins - hands-on time!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8CYWuazzxEl"
      },
      "source": [
        "## 📊 Step 1: Load Our Movie Review Data\n",
        "\n",
        "### What We're Loading:\n",
        "- **IMDB Dataset**: 50,000 movie reviews\n",
        "- **Labels**: Positive or Negative sentiment\n",
        "- **Goal**: Teach our model to understand movie opinions\n",
        "\n",
        "### Data Loading Priority:\n",
        "1. First try: Load cleaned data from previous session\n",
        "2. Second try: Load original IMDB data\n",
        "3. Fallback: Create sample data for demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yJrbrI3zxEm"
      },
      "outputs": [],
      "source": [
        "# 📊 DATA LOADING CELL - Get our movie reviews ready\n",
        "\n",
        "print(\"📂 Loading movie review data...\\n\")\n",
        "\n",
        "# We'll try three different ways to get data\n",
        "data_loaded = False\n",
        "\n",
        "# ATTEMPT 1: Try to load cleaned data from previous session\n",
        "try:\n",
        "    df = pd.read_csv('/content/IMDB_cleaned.csv')\n",
        "    print(\"✅ Excellent! Found cleaned data from data cleaning session!\")\n",
        "    print(\"   This data is already preprocessed and ready.\")\n",
        "    text_column = 'cleaned_review'  # Use the cleaned text column\n",
        "    data_loaded = True\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ No cleaned data found, trying original dataset...\")\n",
        "\n",
        "# ATTEMPT 2: Try to load original IMDB dataset\n",
        "if not data_loaded:\n",
        "    try:\n",
        "        df = pd.read_csv('/content/IMDB Dataset.csv')\n",
        "        print(\"✅ Found original IMDB dataset!\")\n",
        "        print(\"   Note: Using raw reviews (not cleaned)\")\n",
        "        text_column = 'review'  # Use the raw review column\n",
        "        data_loaded = True\n",
        "    except FileNotFoundError:\n",
        "        print(\"❌ No IMDB dataset found either...\")\n",
        "\n",
        "# ATTEMPT 3: Create sample data for demonstration\n",
        "if not data_loaded:\n",
        "    print(\"⚠️ Creating sample data for demonstration...\")\n",
        "    print(\"   (Upload 'IMDB Dataset.csv' for real training)\\n\")\n",
        "\n",
        "    # Create diverse sample reviews\n",
        "    positive_reviews = [\n",
        "        \"This movie was absolutely fantastic! Best film I've seen all year!\",\n",
        "        \"Amazing cinematography and brilliant acting. A masterpiece!\",\n",
        "        \"I loved every minute of it. Highly recommend to everyone!\",\n",
        "        \"Outstanding performance by the lead actor. Oscar-worthy!\",\n",
        "        \"The story was captivating from start to finish. Brilliant!\",\n",
        "        \"One of the best movies ever made. Simply perfect!\",\n",
        "        \"Incredible visual effects and great storyline!\",\n",
        "        \"This film touched my heart. Beautiful and moving.\",\n",
        "    ]\n",
        "\n",
        "    negative_reviews = [\n",
        "        \"Terrible movie. Complete waste of time and money.\",\n",
        "        \"Boring plot and awful acting. Fell asleep halfway through.\",\n",
        "        \"Worst movie I've ever seen. Don't waste your time.\",\n",
        "        \"Poor character development and predictable story.\",\n",
        "        \"The dialogue was cringe-worthy. Couldn't finish it.\",\n",
        "        \"Disappointing. Expected much better from this director.\",\n",
        "        \"Slow pacing and confusing plot. Not recommended.\",\n",
        "        \"Acting was wooden and story made no sense.\",\n",
        "    ]\n",
        "\n",
        "    # Combine and repeat to have more samples\n",
        "    all_reviews = []\n",
        "    all_sentiments = []\n",
        "\n",
        "    # Repeat each review to create a larger dataset\n",
        "    for _ in range(50):  # Repeat 50 times\n",
        "        all_reviews.extend(positive_reviews)\n",
        "        all_reviews.extend(negative_reviews)\n",
        "        all_sentiments.extend(['positive'] * len(positive_reviews))\n",
        "        all_sentiments.extend(['negative'] * len(negative_reviews))\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'review': all_reviews,\n",
        "        'sentiment': all_sentiments\n",
        "    })\n",
        "    text_column = 'review'\n",
        "    print(f\"✅ Created {len(df)} sample reviews for demonstration\")\n",
        "\n",
        "# Display dataset information\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 DATASET INFORMATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total reviews: {len(df):,}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"Text column being used: '{text_column}'\")\n",
        "\n",
        "# Show sentiment distribution\n",
        "print(\"\\n🎭 Sentiment Distribution:\")\n",
        "sentiment_counts = df['sentiment'].value_counts()\n",
        "for sentiment, count in sentiment_counts.items():\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"  {sentiment}: {count:,} reviews ({percentage:.1f}%)\")\n",
        "\n",
        "# Show sample reviews\n",
        "print(\"\\n📝 Sample Reviews:\")\n",
        "print(\"-\" * 60)\n",
        "for i in range(min(3, len(df))):\n",
        "    review_text = df.iloc[i][text_column]\n",
        "    sentiment = df.iloc[i]['sentiment']\n",
        "    # Show first 80 characters of each review\n",
        "    display_text = review_text[:80] + \"...\" if len(review_text) > 80 else review_text\n",
        "    print(f\"Review {i+1} ({sentiment}): {display_text}\")\n",
        "print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMW_d3JfzxEm"
      },
      "source": [
        "## 🔢 Convert Sentiments to Numbers\n",
        "\n",
        "### Why Convert to Numbers?\n",
        "\n",
        "Computers don't understand \"positive\" or \"negative\" - they only understand numbers!\n",
        "\n",
        "So we convert:\n",
        "- **\"negative\"** → 0\n",
        "- **\"positive\"** → 1\n",
        "\n",
        "It's like translating English to Computer language! 🤖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G56F7Qm2zxEn"
      },
      "outputs": [],
      "source": [
        "# 🔢 LABEL CONVERSION CELL - Convert sentiments to numbers\n",
        "\n",
        "print(\"🔄 Converting sentiments to numerical labels...\\n\")\n",
        "\n",
        "# Create a mapping: negative=0, positive=1\n",
        "label_mapping = {\n",
        "    'negative': 0,\n",
        "    'positive': 1\n",
        "}\n",
        "\n",
        "# Apply the mapping to create a new 'label' column\n",
        "df['label'] = df['sentiment'].map(label_mapping)\n",
        "\n",
        "# Verify the conversion worked\n",
        "print(\"✅ Conversion complete!\\n\")\n",
        "print(\"📊 Label Mapping:\")\n",
        "print(\"  'negative' → 0\")\n",
        "print(\"  'positive' → 1\")\n",
        "\n",
        "# Show example of the conversion\n",
        "print(\"\\n📝 Example of conversion:\")\n",
        "print(\"-\" * 50)\n",
        "sample_df = df[['sentiment', 'label']].head(5)\n",
        "for idx, row in sample_df.iterrows():\n",
        "    print(f\"  {row['sentiment']} → {row['label']}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Check for any issues\n",
        "if df['label'].isna().any():\n",
        "    print(\"\\n⚠️ Warning: Some labels couldn't be converted!\")\n",
        "    print(f\"   Missing labels: {df['label'].isna().sum()}\")\n",
        "else:\n",
        "    print(\"\\n✅ All sentiments successfully converted to numbers!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgttj2-xzxEn"
      },
      "source": [
        "## 📉 Select a Sample for Faster Training\n",
        "\n",
        "### Why Use a Sample?\n",
        "\n",
        "For learning purposes, we'll use a smaller sample:\n",
        "- **Full dataset**: 50,000 reviews → Hours of training\n",
        "- **Our sample**: 2,000 reviews → Minutes of training\n",
        "\n",
        "In real production, you'd use all the data!\n",
        "\n",
        "### The Tradeoff:\n",
        "- **More Data** = Better accuracy but slower\n",
        "- **Less Data** = Faster but slightly less accurate\n",
        "\n",
        "For learning, fast is better! ⚡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Pb784KdzxEn"
      },
      "outputs": [],
      "source": [
        "# 📉 SAMPLING CELL - Take a subset for faster training\n",
        "\n",
        "print(\"📊 Preparing data sample for training...\\n\")\n",
        "\n",
        "# Decide how many samples to use\n",
        "# You can change this number!\n",
        "SAMPLE_SIZE = min(2000, len(df))  # Use 2000 or all data if less than 2000\n",
        "\n",
        "print(f\"Original dataset size: {len(df):,} reviews\")\n",
        "print(f\"Sample size for training: {SAMPLE_SIZE:,} reviews\")\n",
        "print(f\"Sampling ratio: {(SAMPLE_SIZE/len(df)*100):.1f}%\\n\")\n",
        "\n",
        "# Take a random sample, ensuring balanced sentiments\n",
        "# random_state=42 ensures reproducible results\n",
        "df_sample = df.sample(n=SAMPLE_SIZE, random_state=42)\n",
        "\n",
        "# Check the balance of our sample\n",
        "positive_count = (df_sample['label'] == 1).sum()\n",
        "negative_count = (df_sample['label'] == 0).sum()\n",
        "\n",
        "print(\"🎭 Sample Distribution:\")\n",
        "print(f\"  Positive reviews: {positive_count} ({positive_count/SAMPLE_SIZE*100:.1f}%)\")\n",
        "print(f\"  Negative reviews: {negative_count} ({negative_count/SAMPLE_SIZE*100:.1f}%)\")\n",
        "\n",
        "# Check if balanced\n",
        "balance_ratio = min(positive_count, negative_count) / max(positive_count, negative_count)\n",
        "if balance_ratio > 0.8:\n",
        "    print(\"\\n✅ Good balance! The sample has similar amounts of positive and negative.\")\n",
        "else:\n",
        "    print(\"\\n⚠️ Sample is imbalanced. Model might be biased.\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_reviews = df_sample[text_column].isna().sum()\n",
        "if missing_reviews > 0:\n",
        "    print(f\"\\n⚠️ Found {missing_reviews} missing reviews. Removing them...\")\n",
        "    df_sample = df_sample.dropna(subset=[text_column])\n",
        "    print(f\"✅ Cleaned! Final sample size: {len(df_sample)}\")\n",
        "else:\n",
        "    print(\"\\n✅ No missing values found!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"📊 Final sample ready: {len(df_sample)} reviews\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M4RnLFVzxEo"
      },
      "source": [
        "## 🔪 Step 2: Split Data into Training and Testing Sets\n",
        "\n",
        "### Why Split the Data?\n",
        "\n",
        "Imagine you're teaching someone math:\n",
        "1. **Training Set (80%)**: Problems they study and practice with\n",
        "2. **Test Set (20%)**: New problems for the final exam\n",
        "\n",
        "We need to test on NEW data the model hasn't seen!\n",
        "\n",
        "### The Split:\n",
        "```\n",
        "All Data (100%)\n",
        "    |\n",
        "    ├── Training Set (80%) - For learning\n",
        "    |\n",
        "    └── Test Set (20%) - For evaluation\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CF9dkFJzxEo"
      },
      "outputs": [],
      "source": [
        "# 🔪 DATA SPLITTING CELL - Divide into train and test\n",
        "\n",
        "print(\"✂️ Splitting data into training and testing sets...\\n\")\n",
        "\n",
        "# Extract texts and labels from our sample\n",
        "texts = df_sample[text_column].tolist()  # Convert to list\n",
        "labels = df_sample['label'].tolist()      # Convert to list\n",
        "\n",
        "print(f\"Total texts: {len(texts)}\")\n",
        "print(f\"Total labels: {len(labels)}\\n\")\n",
        "\n",
        "# Split the data: 80% for training, 20% for testing\n",
        "# stratify ensures both sets have same ratio of positive/negative\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts,                # Our review texts\n",
        "    labels,               # Our labels (0 or 1)\n",
        "    test_size=0.2,        # 20% for testing\n",
        "    random_state=42,      # For reproducible results\n",
        "    stratify=labels       # Keep same positive/negative ratio\n",
        ")\n",
        "\n",
        "# Display the split results\n",
        "print(\"📊 DATA SPLIT COMPLETE!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\n🏋️ TRAINING SET:\")\n",
        "print(f\"  Total samples: {len(train_texts)}\")\n",
        "print(f\"  Positive reviews: {sum(train_labels)} ({sum(train_labels)/len(train_labels)*100:.1f}%)\")\n",
        "print(f\"  Negative reviews: {len(train_labels) - sum(train_labels)} ({(len(train_labels) - sum(train_labels))/len(train_labels)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n🧪 TEST SET:\")\n",
        "print(f\"  Total samples: {len(test_texts)}\")\n",
        "print(f\"  Positive reviews: {sum(test_labels)} ({sum(test_labels)/len(test_labels)*100:.1f}%)\")\n",
        "print(f\"  Negative reviews: {len(test_labels) - sum(test_labels)} ({(len(test_labels) - sum(test_labels))/len(test_labels)*100:.1f}%)\")\n",
        "\n",
        "# Show example from each set\n",
        "print(\"\\n📝 Example from TRAINING set:\")\n",
        "print(f\"  Text: \\\"{train_texts[0][:100]}...\\\"\")\n",
        "print(f\"  Label: {train_labels[0]} ({'positive' if train_labels[0] == 1 else 'negative'})\")\n",
        "\n",
        "print(\"\\n📝 Example from TEST set:\")\n",
        "print(f\"  Text: \\\"{test_texts[0][:100]}...\\\"\")\n",
        "print(f\"  Label: {test_labels[0]} ({'positive' if test_labels[0] == 1 else 'negative'})\")\n",
        "\n",
        "print(\"\\n💡 Remember: The model will NEVER see the test set during training!\")\n",
        "print(\"   We keep it hidden to fairly evaluate performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lV-Mke-zxEp"
      },
      "source": [
        "## 📊 Visualize the Data Split\n",
        "\n",
        "Let's see our data split visually!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX3RNIEizxEp"
      },
      "outputs": [],
      "source": [
        "# 📊 VISUALIZATION CELL - Show the split graphically\n",
        "\n",
        "print(\"📊 Creating visualization of data split...\\n\")\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Define colors\n",
        "colors = ['#e74c3c', '#2ecc71']  # Red for negative, Green for positive\n",
        "\n",
        "# Plot 1: Overall split\n",
        "split_sizes = [len(train_texts), len(test_texts)]\n",
        "axes[0].pie(split_sizes, labels=['Train (80%)', 'Test (20%)'],\n",
        "            autopct='%1.0f%%', colors=['#3498db', '#9b59b6'],\n",
        "            startangle=90)\n",
        "axes[0].set_title('Overall Data Split', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot 2: Training set distribution\n",
        "train_pos = sum(train_labels)\n",
        "train_neg = len(train_labels) - train_pos\n",
        "axes[1].pie([train_neg, train_pos], labels=['Negative', 'Positive'],\n",
        "            autopct='%1.0f%%', colors=colors, startangle=90)\n",
        "axes[1].set_title('Training Set Sentiment', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot 3: Test set distribution\n",
        "test_pos = sum(test_labels)\n",
        "test_neg = len(test_labels) - test_pos\n",
        "axes[2].pie([test_neg, test_pos], labels=['Negative', 'Positive'],\n",
        "            autopct='%1.0f%%', colors=colors, startangle=90)\n",
        "axes[2].set_title('Test Set Sentiment', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.suptitle('📊 Data Split Visualization', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Visualization complete!\")\n",
        "print(\"📝 Note: Both sets have similar sentiment distributions - this is good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_52EY_RMzxEq"
      },
      "source": [
        "## 🤖 Step 3: Load the Pre-trained Model\n",
        "\n",
        "### What's Happening Here?\n",
        "\n",
        "We're downloading a model that already knows English!\n",
        "\n",
        "Think of it like:\n",
        "1. **Hiring an English teacher** (pre-trained model)\n",
        "2. **Teaching them about movies** (fine-tuning)\n",
        "3. **They become a movie critic** (specialized model)\n",
        "\n",
        "### Model Components:\n",
        "\n",
        "1. **Tokenizer**: Converts text → numbers\n",
        "   - \"great movie\" → [2307, 3185]\n",
        "   \n",
        "2. **Model**: The actual brain\n",
        "   - Takes numbers, outputs predictions\n",
        "\n",
        "Let's load them! 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4hvLxtSzxEq"
      },
      "outputs": [],
      "source": [
        "# 🤖 MODEL LOADING CELL - Download and prepare the pre-trained model\n",
        "\n",
        "print(\"🤖 LOADING PRE-TRAINED MODEL\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Choose which model to use\n",
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "\n",
        "print(f\"📦 Model selected: {MODEL_NAME}\")\n",
        "print(\"\\n📝 About this model:\")\n",
        "print(\"  - Created by: Hugging Face\")\n",
        "print(\"  - Size: ~250 MB\")\n",
        "print(\"  - Parameters: 66 million\")\n",
        "print(\"  - Language: English\")\n",
        "print(\"  - Special: 40% smaller than BERT, 97% as good!\")\n",
        "\n",
        "print(\"\\n⬇️ Downloading model components...\")\n",
        "print(\"  (This may take 1-2 minutes on first run)\\n\")\n",
        "\n",
        "# STEP 1: Load the tokenizer\n",
        "print(\"📝 Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(\"✅ Tokenizer loaded successfully!\")\n",
        "print(f\"   Vocabulary size: {tokenizer.vocab_size:,} words\")\n",
        "\n",
        "# STEP 2: Load the model\n",
        "print(\"\\n🧠 Loading model...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2,  # We have 2 classes: positive and negative\n",
        "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},  # Map numbers to labels\n",
        "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}   # Map labels to numbers\n",
        ")\n",
        "print(\"✅ Model loaded successfully!\")\n",
        "\n",
        "# STEP 3: Move model to appropriate device (GPU or CPU)\n",
        "print(f\"\\n🚀 Moving model to {device}...\")\n",
        "model = model.to(device)\n",
        "print(f\"✅ Model is now on {device}\")\n",
        "\n",
        "# Calculate model statistics\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"\\n📊 MODEL STATISTICS:\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
        "print(f\"Model size in memory: ~{total_params * 4 / 1024**2:.1f} MB\")\n",
        "\n",
        "print(\"\\n✨ Model is ready for fine-tuning!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A39A_8DjzxEq"
      },
      "source": [
        "## 🔤 Understanding Tokenization\n",
        "\n",
        "### What is Tokenization?\n",
        "\n",
        "Computers don't understand words - only numbers! Tokenization converts:\n",
        "\n",
        "```\n",
        "\"This movie is great!\"\n",
        "         ↓\n",
        "[101, 2023, 3185, 2003, 2307, 999, 102]\n",
        "```\n",
        "\n",
        "Each number represents a word or part of a word:\n",
        "- 101 = [START]\n",
        "- 2023 = \"this\"\n",
        "- 3185 = \"movie\"\n",
        "- 2003 = \"is\"\n",
        "- 2307 = \"great\"\n",
        "- 999 = \"!\"\n",
        "- 102 = [END]\n",
        "\n",
        "Let's see it in action! 👀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa-MP-KczxEr"
      },
      "outputs": [],
      "source": [
        "# 🔤 TOKENIZATION DEMO CELL - See how text becomes numbers\n",
        "\n",
        "print(\"🔤 TOKENIZATION DEMONSTRATION\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Example sentences to tokenize\n",
        "example_sentences = [\n",
        "    \"This movie is amazing!\",\n",
        "    \"Terrible film\",\n",
        "    \"😍 Best movie ever!!! #MustWatch\"\n",
        "]\n",
        "\n",
        "print(\"Let's see how the tokenizer converts text to numbers:\\n\")\n",
        "\n",
        "for i, sentence in enumerate(example_sentences, 1):\n",
        "    print(f\"Example {i}: \\\"{sentence}\\\"\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    tokens = tokenizer(\n",
        "        sentence,\n",
        "        padding=True,      # Add padding if needed\n",
        "        truncation=True,   # Cut if too long\n",
        "        return_tensors='pt'  # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Get the token IDs\n",
        "    token_ids = tokens['input_ids'][0].tolist()\n",
        "\n",
        "    # Decode each token\n",
        "    print(\"  Token breakdown:\")\n",
        "    for token_id in token_ids:\n",
        "        word = tokenizer.decode([token_id])\n",
        "        # Clean up special tokens for display\n",
        "        if word in ['[CLS]', '[SEP]', '[PAD]']:\n",
        "            print(f\"    {token_id:5d} → {word} (special token)\")\n",
        "        else:\n",
        "            print(f\"    {token_id:5d} → '{word}'\")\n",
        "\n",
        "    print(f\"\\n  Total tokens: {len(token_ids)}\")\n",
        "    print(f\"  Full decoded: \\\"{tokenizer.decode(token_ids)}\\\"\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(\"💡 Note: [CLS] = start, [SEP] = end, [PAD] = padding\")\n",
        "print(\"   These special tokens help the model understand structure!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EBIHgSpzxEr"
      },
      "source": [
        "## 🔢 Step 4: Tokenize All Our Data\n",
        "\n",
        "### Now Let's Convert ALL Our Reviews!\n",
        "\n",
        "We need to tokenize:\n",
        "- Training reviews → numbers for learning\n",
        "- Test reviews → numbers for evaluation\n",
        "\n",
        "### Important Settings:\n",
        "- **max_length=256**: Maximum tokens per review\n",
        "  - Too short = lose information\n",
        "  - Too long = slow training\n",
        "- **padding=True**: Make all reviews same length\n",
        "- **truncation=True**: Cut reviews that are too long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eCVZX-FzxEr"
      },
      "outputs": [],
      "source": [
        "# 🔢 TOKENIZATION CELL - Convert all texts to numbers\n",
        "\n",
        "print(\"🔄 TOKENIZING ALL DATA\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define tokenization function\n",
        "def tokenize_function(texts):\n",
        "    \"\"\"\n",
        "    Convert texts to tokens that the model can understand.\n",
        "\n",
        "    Args:\n",
        "        texts: List of review texts\n",
        "    Returns:\n",
        "        Dictionary with input_ids and attention_mask\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding=True,      # Add padding to make all same length\n",
        "        truncation=True,   # Cut texts that are too long\n",
        "        max_length=256     # Maximum length (in tokens)\n",
        "    )\n",
        "\n",
        "# TOKENIZE TRAINING DATA\n",
        "print(\"📝 Tokenizing training data...\")\n",
        "print(f\"   Processing {len(train_texts)} reviews...\")\n",
        "train_encodings = tokenize_function(train_texts)\n",
        "print(\"✅ Training data tokenized!\")\n",
        "print(f\"   Shape: {len(train_encodings['input_ids'])} reviews\")\n",
        "print(f\"   Max length: {len(train_encodings['input_ids'][0])} tokens\\n\")\n",
        "\n",
        "# TOKENIZE TEST DATA\n",
        "print(\"📝 Tokenizing test data...\")\n",
        "print(f\"   Processing {len(test_texts)} reviews...\")\n",
        "test_encodings = tokenize_function(test_texts)\n",
        "print(\"✅ Test data tokenized!\")\n",
        "print(f\"   Shape: {len(test_encodings['input_ids'])} reviews\")\n",
        "print(f\"   Max length: {len(test_encodings['input_ids'][0])} tokens\\n\")\n",
        "\n",
        "# Show what the tokenizer created\n",
        "print(\"📊 What the tokenizer created:\")\n",
        "print(\"  1. input_ids: The token numbers\")\n",
        "print(\"  2. attention_mask: Which tokens to pay attention to\")\n",
        "print(\"     (1 = real token, 0 = padding)\\n\")\n",
        "\n",
        "# Example of tokenized data\n",
        "print(\"📝 Example of tokenized review:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Original: \\\"{train_texts[0][:100]}...\\\"\")\n",
        "print(f\"\\nTokenized (first 20 tokens):\")\n",
        "print(f\"  {train_encodings['input_ids'][0][:20]}\")\n",
        "print(f\"\\nAttention mask (first 20):\")\n",
        "print(f\"  {train_encodings['attention_mask'][0][:20]}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"\\n✨ All data successfully tokenized and ready for training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_MvZ6nbzxEs"
      },
      "source": [
        "## 📦 Create Dataset Objects\n",
        "\n",
        "### Why Dataset Objects?\n",
        "\n",
        "The Hugging Face library needs data in a special format.\n",
        "Think of it like packaging your data in the right box for shipping! 📦\n",
        "\n",
        "Each dataset contains:\n",
        "- **input_ids**: The tokenized text\n",
        "- **attention_mask**: Which parts to focus on\n",
        "- **labels**: The correct answers (0 or 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6pSNtUBzxEs"
      },
      "outputs": [],
      "source": [
        "# 📦 DATASET CREATION CELL - Package data for training\n",
        "\n",
        "print(\"📦 Creating Dataset Objects\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create training dataset\n",
        "print(\"🏋️ Creating training dataset...\")\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'input_ids': train_encodings['input_ids'],        # The tokenized text\n",
        "    'attention_mask': train_encodings['attention_mask'],  # What to pay attention to\n",
        "    'labels': train_labels                            # The correct answers\n",
        "})\n",
        "print(f\"✅ Training dataset created with {len(train_dataset)} samples\")\n",
        "\n",
        "# Create test dataset\n",
        "print(\"\\n🧪 Creating test dataset...\")\n",
        "test_dataset = Dataset.from_dict({\n",
        "    'input_ids': test_encodings['input_ids'],\n",
        "    'attention_mask': test_encodings['attention_mask'],\n",
        "    'labels': test_labels\n",
        "})\n",
        "print(f\"✅ Test dataset created with {len(test_dataset)} samples\")\n",
        "\n",
        "# Show dataset structure\n",
        "print(\"\\n📊 Dataset Structure:\")\n",
        "print(\"-\" * 40)\n",
        "print(\"Each sample contains:\")\n",
        "print(\"  • input_ids: Token numbers\")\n",
        "print(\"  • attention_mask: Focus indicators\")\n",
        "print(\"  • labels: Correct sentiment (0 or 1)\")\n",
        "\n",
        "# Show an example from the dataset\n",
        "print(\"\\n📝 Example from training dataset:\")\n",
        "print(\"-\" * 40)\n",
        "sample = train_dataset[0]\n",
        "print(f\"Label: {sample['labels']} ({'POSITIVE' if sample['labels'] == 1 else 'NEGATIVE'})\")\n",
        "print(f\"Input IDs (first 10): {sample['input_ids'][:10]}...\")\n",
        "print(f\"Attention (first 10): {sample['attention_mask'][:10]}...\")\n",
        "\n",
        "print(\"\\n✅ Datasets are ready for training!\")\n",
        "print(\"   These will be fed to the model during training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdSB7bcazxEt"
      },
      "source": [
        "## ⚙️ Step 5: Configure Training Settings\n",
        "\n",
        "### 🎛️ Training Parameters Explained:\n",
        "\n",
        "Think of these like recipe instructions:\n",
        "\n",
        "| Parameter | What it means | Our Value | Analogy |\n",
        "|-----------|---------------|-----------|----------|\n",
        "| **Epochs** | How many times to see all data | 3 | Reading a book 3 times |\n",
        "| **Batch Size** | Samples processed together | 16 | Students in a class |\n",
        "| **Learning Rate** | How fast to learn | 2e-5 | Walking speed vs running |\n",
        "| **Warmup Steps** | Gentle start | 500 | Stretching before exercise |\n",
        "\n",
        "### 📊 What Happens During Training:\n",
        "\n",
        "```\n",
        "Epoch 1: See all data once → Learn basics\n",
        "Epoch 2: See all data again → Improve understanding  \n",
        "Epoch 3: See all data again → Perfect the knowledge\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KmXRvWTzxEt"
      },
      "outputs": [],
      "source": [
        "# ⚙️ TRAINING CONFIGURATION CELL - Set all training parameters\n",
        "\n",
        "print(\"⚙️ CONFIGURING TRAINING PARAMETERS\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define all training arguments\n",
        "training_args = TrainingArguments(\n",
        "    # WHERE TO SAVE\n",
        "    output_dir='./results',              # Where to save model checkpoints\n",
        "\n",
        "    # TRAINING SETTINGS\n",
        "    num_train_epochs=3,                  # How many times to see all data\n",
        "    per_device_train_batch_size=16,      # How many samples to process together\n",
        "    per_device_eval_batch_size=32,       # Batch size for evaluation (can be larger)\n",
        "\n",
        "    # LEARNING SETTINGS\n",
        "    warmup_steps=500,                    # Steps for gradual learning rate increase\n",
        "    weight_decay=0.01,                   # Regularization to prevent overfitting\n",
        "    learning_rate=2e-5,                  # How fast to learn (0.00002)\n",
        "\n",
        "    # LOGGING SETTINGS\n",
        "    logging_dir='./logs',                # Where to save training logs\n",
        "    logging_steps=10,                    # Log every 10 steps\n",
        "\n",
        "    # EVALUATION SETTINGS\n",
        "    evaluation_strategy=\"epoch\",         # Evaluate after each epoch\n",
        "    save_strategy=\"epoch\",                # Save model after each epoch\n",
        "\n",
        "    # BEST MODEL SETTINGS\n",
        "    load_best_model_at_end=True,         # Load the best model at the end\n",
        "    metric_for_best_model=\"eval_loss\",   # What metric to use for \"best\"\n",
        "    greater_is_better=False,              # Lower loss is better\n",
        "\n",
        "    # OTHER SETTINGS\n",
        "    push_to_hub=False,                   # Don't auto-upload (we'll do manually)\n",
        "    report_to=\"none\",                    # Don't use tracking tools\n",
        ")\n",
        "\n",
        "# Calculate training statistics\n",
        "total_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
        "\n",
        "print(\"📊 TRAINING CONFIGURATION SUMMARY:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"📚 Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"📦 Batch size: {training_args.per_device_train_batch_size} samples\")\n",
        "print(f\"🎯 Learning rate: {training_args.learning_rate} (0.00002)\")\n",
        "print(f\"🔥 Warmup steps: {training_args.warmup_steps}\")\n",
        "print(f\"📈 Total training steps: ~{total_training_steps}\")\n",
        "\n",
        "print(\"\\n⏱️ ESTIMATED TRAINING TIME:\")\n",
        "if device.type == 'cuda':\n",
        "    estimated_time = total_training_steps * 0.5 / 60  # ~0.5 seconds per step on GPU\n",
        "    print(f\"  With GPU: ~{estimated_time:.1f} minutes\")\n",
        "else:\n",
        "    estimated_time = total_training_steps * 2 / 60  # ~2 seconds per step on CPU\n",
        "    print(f\"  With CPU: ~{estimated_time:.1f} minutes\")\n",
        "\n",
        "print(\"\\n💾 SAVING STRATEGY:\")\n",
        "print(f\"  Model will be saved after each epoch to: {training_args.output_dir}\")\n",
        "print(f\"  Best model will be selected based on: {training_args.metric_for_best_model}\")\n",
        "\n",
        "print(\"\\n✅ Training configuration complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVgCy4G0zxEt"
      },
      "source": [
        "## 📏 Step 6: Define How to Measure Success\n",
        "\n",
        "### Metrics We'll Track:\n",
        "\n",
        "1. **Accuracy**: What percentage did we get right?\n",
        "   - Example: 90% = 9 out of 10 correct\n",
        "   \n",
        "2. **Loss**: How wrong were our predictions?\n",
        "   - Lower is better\n",
        "   - Like a golf score!\n",
        "\n",
        "### Success Criteria:\n",
        "- **Excellent**: >90% accuracy\n",
        "- **Good**: 80-90% accuracy\n",
        "- **Okay**: 70-80% accuracy\n",
        "- **Needs work**: <70% accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwOcd_FRzxEu"
      },
      "outputs": [],
      "source": [
        "# 📏 METRICS CELL - Define how to measure model performance\n",
        "\n",
        "print(\"📏 DEFINING EVALUATION METRICS\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Calculate metrics to evaluate model performance.\n",
        "\n",
        "    Args:\n",
        "        eval_pred: Contains predictions and true labels\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with calculated metrics\n",
        "    \"\"\"\n",
        "    # Extract predictions and labels\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Get the predicted class (highest probability)\n",
        "    # predictions shape: (num_samples, 2) - probabilities for each class\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    # Return metrics\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "    }\n",
        "\n",
        "print(\"✅ Metrics function defined!\\n\")\n",
        "\n",
        "print(\"📊 Metrics we'll track during training:\")\n",
        "print(\"-\" * 40)\n",
        "print(\"1. ACCURACY:\")\n",
        "print(\"   • What it measures: Percentage of correct predictions\")\n",
        "print(\"   • Range: 0% to 100%\")\n",
        "print(\"   • Goal: As high as possible!\")\n",
        "print(\"\\n2. LOSS:\")\n",
        "print(\"   • What it measures: How wrong the predictions are\")\n",
        "print(\"   • Range: 0 to infinity\")\n",
        "print(\"   • Goal: As low as possible!\")\n",
        "\n",
        "print(\"\\n🎯 Success Benchmarks:\")\n",
        "print(\"   >95% accuracy = 🏆 Outstanding!\")\n",
        "print(\"   90-95% = 🎉 Excellent\")\n",
        "print(\"   85-90% = 😊 Very Good\")\n",
        "print(\"   80-85% = 👍 Good\")\n",
        "print(\"   <80% = 📈 Room for improvement\")\n",
        "\n",
        "print(\"\\n✅ Ready to evaluate model performance!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHeffmf9zxEu"
      },
      "source": [
        "## 🎓 Step 7: Create the Trainer\n",
        "\n",
        "### What is the Trainer?\n",
        "\n",
        "The Trainer is like a personal coach for your model:\n",
        "- 📚 Shows training examples\n",
        "- 📝 Tests understanding\n",
        "- 📈 Tracks progress\n",
        "- 💾 Saves best version\n",
        "- 🎯 Optimizes learning\n",
        "\n",
        "It handles all the complex training logic for us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC03hAZjzxEu"
      },
      "outputs": [],
      "source": [
        "# 🎓 TRAINER CREATION CELL - Set up the training manager\n",
        "\n",
        "print(\"🎓 CREATING THE TRAINER\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"📦 Assembling all components...\\n\")\n",
        "\n",
        "# Create the Trainer with all our components\n",
        "trainer = Trainer(\n",
        "    model=model,                          # Our DistilBERT model\n",
        "    args=training_args,                   # Training configuration\n",
        "    train_dataset=train_dataset,          # Training data\n",
        "    eval_dataset=test_dataset,            # Test data for evaluation\n",
        "    tokenizer=tokenizer,                  # Tokenizer for processing text\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # Handles padding\n",
        "    compute_metrics=compute_metrics,      # Our metrics function\n",
        ")\n",
        "\n",
        "print(\"✅ Trainer successfully created!\\n\")\n",
        "\n",
        "print(\"📊 TRAINER CONFIGURATION:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"🤖 Model: {MODEL_NAME}\")\n",
        "print(f\"📚 Training samples: {len(train_dataset)}\")\n",
        "print(f\"🧪 Test samples: {len(test_dataset)}\")\n",
        "print(f\"🔄 Epochs to train: {training_args.num_train_epochs}\")\n",
        "print(f\"📦 Batch size: {training_args.per_device_train_batch_size}\")\n",
        "\n",
        "print(\"\\n🎯 What the Trainer will do:\")\n",
        "print(\"  1. Show model training examples\")\n",
        "print(\"  2. Calculate how wrong predictions are\")\n",
        "print(\"  3. Adjust model to improve\")\n",
        "print(\"  4. Repeat for all epochs\")\n",
        "print(\"  5. Save best version\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🚀 READY TO START FINE-TUNING!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n⏱️ Training will begin in the next cell...\")\n",
        "print(\"☕ This is a good time to grab coffee!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxZ21wcBzxEv"
      },
      "source": [
        "## 🚀 Step 8: FINE-TUNE THE MODEL!\n",
        "\n",
        "### This is the Main Event! 🎯\n",
        "\n",
        "What happens during training:\n",
        "1. **Epoch 1**: Model sees all data once, starts learning patterns\n",
        "2. **Epoch 2**: Reinforces learning, improves accuracy\n",
        "3. **Epoch 3**: Fine-tunes understanding, perfects predictions\n",
        "\n",
        "Watch the loss go down and accuracy go up! 📈"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoW1p_t_zxEv"
      },
      "outputs": [],
      "source": [
        "# 🚀 TRAINING CELL - Fine-tune the model!\n",
        "\n",
        "import time\n",
        "\n",
        "print(\"🚀 STARTING FINE-TUNING PROCESS!\\n\")\n",
        "print(\"=\"*60)\n",
        "print(\"📚 Training Details:\")\n",
        "print(f\"  • Model: {MODEL_NAME}\")\n",
        "print(f\"  • Training samples: {len(train_dataset)}\")\n",
        "print(f\"  • Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  • Device: {device}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n⏱️ Starting timer...\")\n",
        "print(\"🔄 Training in progress...\\n\")\n",
        "print(\"You'll see updates every few steps:\")\n",
        "print(\"  • loss = how wrong the model is (lower is better)\")\n",
        "print(\"  • learning_rate = how fast we're learning\")\n",
        "print(\"  • epoch = which round of training\\n\")\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# START TRAINING!\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Calculate training duration\n",
        "training_time = time.time() - start_time\n",
        "minutes = int(training_time // 60)\n",
        "seconds = int(training_time % 60)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎉 FINE-TUNING COMPLETE! 🎉\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n📊 TRAINING SUMMARY:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"⏱️ Total time: {minutes} minutes {seconds} seconds\")\n",
        "print(f\"📉 Final training loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"📈 Total steps: {train_result.global_step}\")\n",
        "print(f\"🔄 Epochs completed: {training_args.num_train_epochs}\")\n",
        "\n",
        "# Performance interpretation\n",
        "if train_result.training_loss < 0.3:\n",
        "    print(\"\\n✨ Excellent! Very low loss - model learned well!\")\n",
        "elif train_result.training_loss < 0.5:\n",
        "    print(\"\\n👍 Good! Reasonable loss - model is performing well!\")\n",
        "else:\n",
        "    print(\"\\n📈 Model trained, but might benefit from more epochs!\")\n",
        "\n",
        "print(\"\\n💾 Model checkpoints saved to: ./results\")\n",
        "print(\"🎯 Best model automatically selected based on validation loss!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyuVNLgbzxE3"
      },
      "source": [
        "## 🧪 Step 9: Evaluate Model Performance\n",
        "\n",
        "### Time to Test Our Model! 🎯\n",
        "\n",
        "Now we test on data the model has NEVER seen before.\n",
        "This is the real test - like a final exam!\n",
        "\n",
        "We'll check:\n",
        "- **Accuracy**: How many did we get right?\n",
        "- **Confusion Matrix**: Where did we make mistakes?\n",
        "- **Classification Report**: Detailed performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPBvQUDhzxE3"
      },
      "outputs": [],
      "source": [
        "# 🧪 EVALUATION CELL - Test the model's performance\n",
        "\n",
        "print(\"🧪 EVALUATING MODEL PERFORMANCE\\n\")\n",
        "print(\"=\"*60)\n",
        "print(\"Testing on unseen data...\\n\")\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Display results\n",
        "print(\"📊 EVALUATION RESULTS:\")\n",
        "print(\"=\"*40)\n",
        "print(f\"✅ Accuracy: {eval_results['eval_accuracy']*100:.2f}%\")\n",
        "print(f\"📉 Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Interpret the results\n",
        "accuracy_percent = eval_results['eval_accuracy'] * 100\n",
        "\n",
        "print(\"\\n🎯 PERFORMANCE INTERPRETATION:\")\n",
        "if accuracy_percent >= 95:\n",
        "    print(f\"🏆 OUTSTANDING! {accuracy_percent:.1f}% accuracy is exceptional!\")\n",
        "    print(\"   Your model is performing at professional level!\")\n",
        "elif accuracy_percent >= 90:\n",
        "    print(f\"🎉 EXCELLENT! {accuracy_percent:.1f}% accuracy is very impressive!\")\n",
        "    print(\"   Your model is ready for production use!\")\n",
        "elif accuracy_percent >= 85:\n",
        "    print(f\"😊 VERY GOOD! {accuracy_percent:.1f}% accuracy is solid!\")\n",
        "    print(\"   Your model is performing well!\")\n",
        "elif accuracy_percent >= 80:\n",
        "    print(f\"👍 GOOD! {accuracy_percent:.1f}% accuracy is respectable!\")\n",
        "    print(\"   Your model has learned the patterns!\")\n",
        "else:\n",
        "    print(f\"📈 ROOM FOR IMPROVEMENT: {accuracy_percent:.1f}% accuracy\")\n",
        "    print(\"   Consider: more data, more epochs, or different parameters\")\n",
        "\n",
        "# Compare to baseline\n",
        "print(\"\\n📊 CONTEXT:\")\n",
        "print(\"  • Random guessing: 50% accuracy\")\n",
        "print(\"  • Untrained model: ~50-60% accuracy\")\n",
        "print(f\"  • Your fine-tuned model: {accuracy_percent:.1f}% accuracy\")\n",
        "print(f\"  • Improvement: +{accuracy_percent - 50:.1f}% over random!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxFx2lUCzxE4"
      },
      "source": [
        "## 📊 Confusion Matrix - Where Did We Make Mistakes?\n",
        "\n",
        "The confusion matrix shows:\n",
        "- **True Negatives**: Correctly predicted negative\n",
        "- **True Positives**: Correctly predicted positive\n",
        "- **False Positives**: Wrongly said positive (was negative)\n",
        "- **False Negatives**: Wrongly said negative (was positive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ2_AbM8zxE4"
      },
      "outputs": [],
      "source": [
        "# 📊 CONFUSION MATRIX CELL - Visualize prediction errors\n",
        "\n",
        "print(\"📊 CREATING CONFUSION MATRIX\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get predictions for test set\n",
        "print(\"🔮 Getting model predictions...\")\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Create visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'],\n",
        "            cbar_kws={'label': 'Count'},\n",
        "            annot_kws={'size': 14})\n",
        "\n",
        "plt.title('Confusion Matrix - How Well Did We Predict?', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.ylabel('True Label (Actual)', fontsize=12)\n",
        "plt.xlabel('Predicted Label (Model\\'s Guess)', fontsize=12)\n",
        "\n",
        "# Add text boxes explaining each quadrant\n",
        "plt.text(0.5, -0.15, 'Perfect diagonal = Perfect predictions!',\n",
        "         ha='center', transform=plt.gca().transAxes, fontsize=10, style='italic')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display metrics from confusion matrix\n",
        "true_neg, false_pos, false_neg, true_pos = cm.ravel()\n",
        "total = true_neg + false_pos + false_neg + true_pos\n",
        "\n",
        "print(\"\\n📊 CONFUSION MATRIX BREAKDOWN:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"✅ True Negatives:  {true_neg:4d} (Correctly predicted negative)\")\n",
        "print(f\"✅ True Positives:  {true_pos:4d} (Correctly predicted positive)\")\n",
        "print(f\"❌ False Positives: {false_pos:4d} (Said positive, was negative)\")\n",
        "print(f\"❌ False Negatives: {false_neg:4d} (Said negative, was positive)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total Correct: {true_neg + true_pos}/{total} ({(true_neg + true_pos)/total*100:.1f}%)\")\n",
        "print(f\"Total Wrong:   {false_pos + false_neg}/{total} ({(false_pos + false_neg)/total*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFdHwBbJzxE5"
      },
      "source": [
        "## 📋 Detailed Performance Report\n",
        "\n",
        "### Metrics Explained:\n",
        "- **Precision**: When we say positive, how often are we right?\n",
        "- **Recall**: Of all actual positives, how many did we find?\n",
        "- **F1-Score**: Balance between precision and recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VKwOj1BzxE5"
      },
      "outputs": [],
      "source": [
        "# 📋 CLASSIFICATION REPORT CELL - Detailed metrics\n",
        "\n",
        "print(\"📋 DETAILED CLASSIFICATION REPORT\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_true, y_pred,\n",
        "                              target_names=['Negative', 'Positive'],\n",
        "                              digits=3)\n",
        "\n",
        "print(report)\n",
        "\n",
        "print(\"\\n📖 HOW TO READ THIS REPORT:\")\n",
        "print(\"=\"*50)\n",
        "print(\"PRECISION: Of all reviews we labeled as X, what % were correct?\")\n",
        "print(\"  • High precision = Few false positives\")\n",
        "print(\"  • Example: 0.90 = 90% of our 'positive' predictions were right\")\n",
        "\n",
        "print(\"\\nRECALL: Of all actual X reviews, what % did we find?\")\n",
        "print(\"  • High recall = We found most of them\")\n",
        "print(\"  • Example: 0.85 = We found 85% of all positive reviews\")\n",
        "\n",
        "print(\"\\nF1-SCORE: Harmonic mean of precision and recall\")\n",
        "print(\"  • Balance between precision and recall\")\n",
        "print(\"  • Higher is better (max = 1.0)\")\n",
        "\n",
        "print(\"\\nSUPPORT: How many samples in each category\")\n",
        "print(\"  • Shows data distribution in test set\")\n",
        "\n",
        "print(\"\\n🎯 QUICK ASSESSMENT:\")\n",
        "# Calculate average F1 score\n",
        "report_dict = classification_report(y_true, y_pred,\n",
        "                                   target_names=['Negative', 'Positive'],\n",
        "                                   output_dict=True)\n",
        "avg_f1 = report_dict['macro avg']['f1-score']\n",
        "\n",
        "if avg_f1 > 0.9:\n",
        "    print(f\"  F1-Score of {avg_f1:.3f} = Excellent balanced performance!\")\n",
        "elif avg_f1 > 0.8:\n",
        "    print(f\"  F1-Score of {avg_f1:.3f} = Good balanced performance!\")\n",
        "else:\n",
        "    print(f\"  F1-Score of {avg_f1:.3f} = Room for improvement\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NltYQnYzxE6"
      },
      "source": [
        "## 🎬 Step 10: Test with Real Movie Reviews!\n",
        "\n",
        "### Let's See Our Model in Action! 🍿\n",
        "\n",
        "Time to test with some real reviews and see how confident our model is!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7ctCB2jzxE6"
      },
      "outputs": [],
      "source": [
        "# 🎬 TESTING CELL - Try the model with custom reviews\n",
        "\n",
        "def predict_sentiment(text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Predict sentiment for any text.\n",
        "    Returns sentiment, confidence, and probability scores.\n",
        "    \"\"\"\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\",\n",
        "                      truncation=True, padding=True,\n",
        "                      max_length=256)\n",
        "\n",
        "    # Move to correct device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get prediction (no gradient calculation needed)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Calculate probabilities\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    prediction = torch.argmax(probs, dim=-1)\n",
        "\n",
        "    # Get confidence score\n",
        "    confidence = probs[0][prediction].item()\n",
        "\n",
        "    # Determine sentiment with emoji\n",
        "    if prediction.item() == 1:\n",
        "        sentiment = \"POSITIVE 😊\"\n",
        "        emoji = \"👍\"\n",
        "    else:\n",
        "        sentiment = \"NEGATIVE 😞\"\n",
        "        emoji = \"👎\"\n",
        "\n",
        "    return sentiment, confidence, probs[0].cpu().numpy(), emoji\n",
        "\n",
        "# Test reviews - mix of easy and challenging\n",
        "test_reviews = [\n",
        "    # Clear positive\n",
        "    \"This movie was absolutely phenomenal! Best film I've seen in years!\",\n",
        "\n",
        "    # Clear negative\n",
        "    \"Terrible waste of time. I want my money back. Worst movie ever.\",\n",
        "\n",
        "    # Subtle positive\n",
        "    \"A thoughtful and well-crafted story that stays with you.\",\n",
        "\n",
        "    # Subtle negative\n",
        "    \"The plot was confusing and the pacing felt off throughout.\",\n",
        "\n",
        "    # Mixed sentiment (challenging)\n",
        "    \"Great acting but terrible script. Not sure how I feel about it.\",\n",
        "\n",
        "    # Very short\n",
        "    \"Loved it!\",\n",
        "\n",
        "    # Sarcastic (challenging)\n",
        "    \"Oh great, another superhero movie. Just what we needed.\",\n",
        "]\n",
        "\n",
        "print(\"🎬 TESTING WITH CUSTOM MOVIE REVIEWS\\n\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, review in enumerate(test_reviews, 1):\n",
        "    sentiment, confidence, probs, emoji = predict_sentiment(review, model, tokenizer)\n",
        "\n",
        "    # Display review\n",
        "    print(f\"Review #{i}:\")\n",
        "    print(f\"📝 \\\"{review}\\\"\")\n",
        "    print()\n",
        "\n",
        "    # Display prediction\n",
        "    print(f\"🤖 Model says: {sentiment} {emoji}\")\n",
        "    print(f\"💪 Confidence: {confidence*100:.1f}%\")\n",
        "\n",
        "    # Visual confidence bar\n",
        "    bar_length = 20\n",
        "    filled = int(bar_length * confidence)\n",
        "    bar = '█' * filled + '░' * (bar_length - filled)\n",
        "    print(f\"📊 [{bar}]\")\n",
        "\n",
        "    # Detailed scores\n",
        "    print(f\"📈 Scores: Negative={probs[0]*100:.1f}% | Positive={probs[1]*100:.1f}%\")\n",
        "\n",
        "    # Interpretation\n",
        "    if confidence > 0.95:\n",
        "        print(\"✨ Very confident prediction!\")\n",
        "    elif confidence > 0.8:\n",
        "        print(\"👍 Confident prediction\")\n",
        "    elif confidence > 0.6:\n",
        "        print(\"🤔 Somewhat uncertain\")\n",
        "    else:\n",
        "        print(\"😕 Very uncertain - could go either way\")\n",
        "\n",
        "    print(\"-\"*70)\n",
        "\n",
        "print(\"\\n💡 Note: Lower confidence on mixed/sarcastic reviews is expected!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGJU9HVpzxE7"
      },
      "source": [
        "## 🎮 Interactive Testing - Try Your Own Reviews!\n",
        "\n",
        "### Your Turn to Test the Model! 🎯\n",
        "\n",
        "Type any movie review and see what the model thinks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC5QBEztzxE7"
      },
      "outputs": [],
      "source": [
        "# 🎮 INTERACTIVE TESTING CELL - User can input their own reviews\n",
        "\n",
        "print(\"🎮 INTERACTIVE SENTIMENT ANALYZER\")\n",
        "print(\"=\"*60)\n",
        "print(\"Test the model with your own movie reviews!\")\n",
        "print(\"Type 'quit' to exit\\n\")\n",
        "\n",
        "# Example prompts to inspire users\n",
        "print(\"💡 Try different types of reviews:\")\n",
        "print(\"  • Clear positive: 'Amazing movie!'\")\n",
        "print(\"  • Clear negative: 'Boring and slow'\")\n",
        "print(\"  • Mixed feelings: 'Good acting but weak plot'\")\n",
        "print(\"  • Your actual opinion about a movie!\\n\")\n",
        "\n",
        "# Interactive loop\n",
        "review_count = 0\n",
        "while review_count < 5:  # Limit to 5 reviews in notebook\n",
        "    user_input = input(f\"\\n👤 Enter review #{review_count + 1} (or 'quit'): \")\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"\\n👋 Thanks for testing the model!\")\n",
        "        break\n",
        "\n",
        "    if len(user_input.strip()) < 3:\n",
        "        print(\"⚠️ Please enter a longer review (at least 3 characters)\")\n",
        "        continue\n",
        "\n",
        "    # Get prediction\n",
        "    sentiment, confidence, probs, emoji = predict_sentiment(user_input, model, tokenizer)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n🤖 ANALYSIS:\")\n",
        "    print(f\"   Sentiment: {sentiment} {emoji}\")\n",
        "    print(f\"   Confidence: {confidence*100:.1f}%\")\n",
        "\n",
        "    # Visual confidence meter\n",
        "    bar_length = 30\n",
        "    filled = int(bar_length * confidence)\n",
        "    bar = '█' * filled + '░' * (bar_length - filled)\n",
        "    print(f\"   Confidence: [{bar}]\")\n",
        "\n",
        "    # Detailed breakdown\n",
        "    print(f\"\\n   📊 Probability Breakdown:\")\n",
        "    print(f\"      Negative: {probs[0]*100:.1f}%\")\n",
        "    print(f\"      Positive: {probs[1]*100:.1f}%\")\n",
        "\n",
        "    review_count += 1\n",
        "\n",
        "    if review_count < 5:\n",
        "        print(\"\\n\" + \"-\"*60)\n",
        "\n",
        "if review_count == 5:\n",
        "    print(\"\\n📝 Reached 5 reviews limit. Restart cell to test more!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🎉 Great job testing the model!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mUtiJSzzxE8"
      },
      "source": [
        "## 💾 Step 11: Save Your Fine-tuned Model\n",
        "\n",
        "### Time to Save Your Work! 📦\n",
        "\n",
        "We'll save:\n",
        "1. The model weights (the learned knowledge)\n",
        "2. The tokenizer (text processor)\n",
        "3. Configuration files\n",
        "\n",
        "This lets you use the model later without retraining!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbjglfoWzxE8"
      },
      "outputs": [],
      "source": [
        "# 💾 SAVE MODEL CELL - Save your fine-tuned model locally\n",
        "\n",
        "print(\"💾 SAVING YOUR FINE-TUNED MODEL\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define save directory\n",
        "save_directory = \"./my_movie_sentiment_model\"\n",
        "\n",
        "print(f\"📁 Save location: {save_directory}\")\n",
        "print(\"\\n📦 Saving components:\")\n",
        "\n",
        "# Save the model\n",
        "print(\"  1. Saving model weights...\")\n",
        "trainer.save_model(save_directory)\n",
        "print(\"     ✅ Model saved!\")\n",
        "\n",
        "# Save the tokenizer\n",
        "print(\"  2. Saving tokenizer...\")\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "print(\"     ✅ Tokenizer saved!\")\n",
        "\n",
        "print(\"\\n✅ Model successfully saved!\\n\")\n",
        "\n",
        "# Check what was saved\n",
        "import os\n",
        "saved_files = os.listdir(save_directory)\n",
        "\n",
        "print(\"📁 SAVED FILES:\")\n",
        "print(\"-\" * 40)\n",
        "total_size = 0\n",
        "for file in saved_files:\n",
        "    file_path = os.path.join(save_directory, file)\n",
        "    file_size = os.path.getsize(file_path) / (1024*1024)  # Convert to MB\n",
        "    total_size += file_size\n",
        "    print(f\"  📄 {file:30s} ({file_size:6.1f} MB)\")\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"  📊 Total size: {total_size:.1f} MB\")\n",
        "\n",
        "print(\"\\n📝 HOW TO LOAD THIS MODEL LATER:\")\n",
        "print(\"```python\")\n",
        "print(\"from transformers import AutoModelForSequenceClassification, AutoTokenizer\")\n",
        "print(f\"model = AutoModelForSequenceClassification.from_pretrained('{save_directory}')\")\n",
        "print(f\"tokenizer = AutoTokenizer.from_pretrained('{save_directory}')\")\n",
        "print(\"```\")\n",
        "\n",
        "print(\"\\n✨ Your model is saved and ready to use anytime!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CswS5MaczxE8"
      },
      "source": [
        "## 🚀 Step 12: Deploy to Hugging Face Hub (Optional)\n",
        "\n",
        "### Share Your Model with the World! 🌍\n",
        "\n",
        "By uploading to Hugging Face:\n",
        "- Anyone can use your model\n",
        "- You get a model card page\n",
        "- Free hosting forever\n",
        "- Version control included\n",
        "\n",
        "### Prerequisites:\n",
        "1. Create account at https://huggingface.co\n",
        "2. Get token from https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od_oUvFzzxE9"
      },
      "outputs": [],
      "source": [
        "# 🚀 DEPLOYMENT CELL - Upload to Hugging Face Hub\n",
        "\n",
        "print(\"🚀 DEPLOYING TO HUGGING FACE HUB\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"📝 SETUP INSTRUCTIONS:\")\n",
        "print(\"1. Go to: https://huggingface.co/join (create account)\")\n",
        "print(\"2. Go to: https://huggingface.co/settings/tokens\")\n",
        "print(\"3. Click 'New token'\")\n",
        "print(\"4. Name it, select 'write' permission\")\n",
        "print(\"5. Copy the token\\n\")\n",
        "\n",
        "# Login to Hugging Face\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "print(\"🔐 Please login to Hugging Face:\")\n",
        "print(\"(Paste your token in the box below)\\n\")\n",
        "\n",
        "# This creates a login widget\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzDJhnMqzxE9"
      },
      "outputs": [],
      "source": [
        "# 📤 UPLOAD MODEL CELL - Push to Hugging Face\n",
        "\n",
        "print(\"📤 UPLOADING MODEL TO HUGGING FACE\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Choose a name for your model\n",
        "model_name = \"my-imdb-sentiment-analyzer\"  # Change this to your preference!\n",
        "\n",
        "print(f\"📦 Model name: {model_name}\")\n",
        "print(\"\\n🌐 Uploading... (this may take 1-2 minutes)\\n\")\n",
        "\n",
        "try:\n",
        "    # Upload model\n",
        "    print(\"📤 Uploading model...\")\n",
        "    model.push_to_hub(model_name, use_temp_dir=True)\n",
        "    print(\"✅ Model uploaded!\")\n",
        "\n",
        "    # Upload tokenizer\n",
        "    print(\"\\n📤 Uploading tokenizer...\")\n",
        "    tokenizer.push_to_hub(model_name, use_temp_dir=True)\n",
        "    print(\"✅ Tokenizer uploaded!\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎉 SUCCESS! Model deployed to Hugging Face Hub!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"\\n🌐 Your model page: https://huggingface.co/YOUR_USERNAME/{model_name}\")\n",
        "    print(\"   (Replace YOUR_USERNAME with your Hugging Face username)\")\n",
        "\n",
        "    print(\"\\n📝 HOW OTHERS CAN USE YOUR MODEL:\")\n",
        "    print(\"```python\")\n",
        "    print(\"from transformers import pipeline\")\n",
        "    print(f\"classifier = pipeline('sentiment-analysis', model='YOUR_USERNAME/{model_name}')\")\n",
        "    print(\"result = classifier('This movie is amazing!')\")\n",
        "    print(\"print(result)\")\n",
        "    print(\"```\")\n",
        "\n",
        "    print(\"\\n🎊 Congratulations! You've deployed an AI model!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n⚠️ Upload failed: {str(e)}\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"1. Make sure you're logged in with a valid token\")\n",
        "    print(\"2. Check your internet connection\")\n",
        "    print(\"3. Verify your token has 'write' permissions\")\n",
        "    print(\"\\n💡 Your model is still saved locally and works fine!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5cLTcIVzxE-"
      },
      "source": [
        "## 🎓 Final Summary - What You've Accomplished!\n",
        "\n",
        "### 🏆 Congratulations! You've Successfully:\n",
        "\n",
        "✅ **Loaded** a pre-trained DistilBERT model  \n",
        "✅ **Prepared** movie review data for training  \n",
        "✅ **Fine-tuned** the model on sentiment analysis  \n",
        "✅ **Achieved** ~{accuracy}% accuracy!  \n",
        "✅ **Tested** with custom reviews  \n",
        "✅ **Saved** your model locally  \n",
        "✅ **Deployed** to Hugging Face (optional)  \n",
        "\n",
        "### 📊 Your Model's Journey:\n",
        "\n",
        "```\n",
        "Start: Generic language model (50% accuracy on sentiment)\n",
        "                    ↓\n",
        "         Fine-tuning with YOUR data\n",
        "                    ↓\n",
        "End: Specialized sentiment expert (~90%+ accuracy!)\n",
        "```\n",
        "\n",
        "### 🔑 Key Skills You've Learned:\n",
        "\n",
        "1. **Transfer Learning** - Using pre-trained models\n",
        "2. **Data Preparation** - Converting text to model format\n",
        "3. **Fine-tuning** - Specializing models for your task\n",
        "4. **Evaluation** - Measuring model performance\n",
        "5. **Deployment** - Sharing models with others\n",
        "\n",
        "### 🚀 What Can You Do Next?\n",
        "\n",
        "#### With Your Current Model:\n",
        "- Build a web app for movie review analysis\n",
        "- Create an API endpoint\n",
        "- Analyze movie review datasets\n",
        "- Add to your portfolio!\n",
        "\n",
        "#### Apply to Your Field (Electrical Engineering):\n",
        "- **Equipment logs**: Classify failure types\n",
        "- **Maintenance reports**: Predict urgency levels\n",
        "- **Customer feedback**: Analyze satisfaction\n",
        "- **Technical docs**: Categorize by topic\n",
        "\n",
        "### 📚 Resources for Continued Learning:\n",
        "\n",
        "1. **Hugging Face Course** (Free!): https://huggingface.co/course\n",
        "2. **Model Hub**: https://huggingface.co/models\n",
        "3. **Documentation**: https://huggingface.co/docs\n",
        "4. **Community**: https://discuss.huggingface.co\n",
        "\n",
        "### 💡 Final Thoughts:\n",
        "\n",
        "> \"You've just trained an AI model that understands human sentiment!\n",
        "> This same technique powers ChatGPT, Google Search, and countless\n",
        "> AI applications. You're now part of the AI revolution!\"\n",
        "\n",
        "### 🎉 Amazing Work! You're Now an AI Practitioner!\n",
        "\n",
        "---\n",
        "\n",
        "## Questions? Let's Discuss! 🙋‍♂️🙋‍♀️\n",
        "\n",
        "Feel free to ask about:\n",
        "- How to improve accuracy\n",
        "- Different model architectures\n",
        "- Applying to your specific use case\n",
        "- Troubleshooting issues\n",
        "- Next steps in your AI journey\n",
        "\n",
        "**Thank you for joining this session!** 🙏"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}