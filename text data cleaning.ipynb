{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎬 Text Data Cleaning for NLP: A Practical Guide\n",
    "\n",
    "## Session Overview\n",
    "Welcome to this hands-on session on **Text Data Cleaning** - a crucial step in any NLP pipeline!\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "By the end of this session, you will:\n",
    "- ✅ Understand why data cleaning is critical for NLP models\n",
    "- ✅ Master essential text cleaning techniques\n",
    "- ✅ Apply these techniques to real-world data (IMDB movie reviews)\n",
    "- ✅ Visualize the impact of data cleaning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 Why Do We Need Data Cleaning?\n",
    "\n",
    "### The Problem with Raw Text\n",
    "\n",
    "Imagine you're teaching someone to read - would you give them:\n",
    "- A book with random symbols? 🔣\n",
    "- Text with spelling errors? ❌\n",
    "- Mixed languages and emojis? 😵\n",
    "\n",
    "**Neither would an AI model learn well from messy data!**\n",
    "\n",
    "### Real-World Example\n",
    "\n",
    "| 🚫 **Raw Text** | ✅ **Clean Text** | 💡 **Why Clean?** |\n",
    "|---|---|---|\n",
    "| \"This movie was AMAZING!!!!! 😍😍 #BestMovieEver @friends\" | \"this movie was amazing\" | Removes noise, normalizes text |\n",
    "| \"I can't believe it's not better... <br><br> Worst film EVER!\" | \"i cannot believe it is not better worst film ever\" | Handles contractions, HTML tags |\n",
    "| \"The    movie  was     okay....\" | \"the movie was okay\" | Removes extra spaces and punctuation |\n",
    "\n",
    "### Impact on Model Performance\n",
    "\n",
    "```\n",
    "Dirty Data → 😰 Confused Model → 📉 Poor Predictions\n",
    "Clean Data → 😊 Happy Model → 📈 Accurate Predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Installing Required Libraries\n",
    "\n",
    "Let's start by installing and importing all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run this once in Colab)\n",
    "!pip install nltk wordcloud contractions beautifulsoup4 -q\n",
    "\n",
    "print(\"✅ Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# For handling contractions and HTML\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(\"📊 Pandas version:\", pd.__version__)\n",
    "print(\"🔤 NLTK version:\", nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Loading the IMDB Dataset\n",
    "\n",
    "We'll use the famous IMDB movie reviews dataset with 50,000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    # Load from Colab\n",
    "    df = pd.read_csv('/content/IMDB-Dataset.csv')\n",
    "    print(\"✅ Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File not found. Please upload IMDB-Dataset.csv to Colab.\")\n",
    "    print(\"You can download it from: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "    raise\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\n📊 Dataset Shape: {df.shape}\")\n",
    "print(f\"📝 Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\n🎬 Dataset Preview:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and basic statistics\n",
    "print(\"🔍 Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total reviews: {len(df)}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"\\nSentiment Distribution:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(\"\\n📏 Review Length Statistics:\")\n",
    "df['review_length'] = df['review'].str.len()\n",
    "print(f\"Average length: {df['review_length'].mean():.0f} characters\")\n",
    "print(f\"Shortest review: {df['review_length'].min()} characters\")\n",
    "print(f\"Longest review: {df['review_length'].max()} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Let's Examine a Raw Review\n",
    "\n",
    "Before cleaning, let's see what our raw data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample review with common issues\n",
    "sample_index = 0\n",
    "sample_review = df.loc[sample_index, 'review']\n",
    "\n",
    "print(\"🎬 Sample Raw Review:\")\n",
    "print(\"=\"*80)\n",
    "print(sample_review[:500] + \"...\" if len(sample_review) > 500 else sample_review)\n",
    "print(\"\\n📊 Characteristics:\")\n",
    "print(f\"- Length: {len(sample_review)} characters\")\n",
    "print(f\"- Contains HTML tags: {'<br' in sample_review}\")\n",
    "print(f\"- Contains numbers: {bool(re.search(r'\\d', sample_review))}\")\n",
    "print(f\"- Contains special characters: {bool(re.search(r'[^a-zA-Z0-9\\s]', sample_review))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Data Cleaning Pipeline\n",
    "\n",
    "Now let's build our cleaning pipeline step by step!\n",
    "\n",
    "### 📋 Our Cleaning Steps:\n",
    "\n",
    "1. **Remove HTML tags** → Clean web scraping artifacts\n",
    "2. **Expand contractions** → \"don't\" → \"do not\"\n",
    "3. **Convert to lowercase** → Normalize text\n",
    "4. **Remove special characters** → Keep only letters and spaces\n",
    "5. **Remove extra spaces** → Clean formatting\n",
    "6. **Remove stopwords** (optional) → Focus on meaningful words\n",
    "7. **Lemmatization** (optional) → Reduce words to root form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Remove HTML Tags 🏷️\n",
    "\n",
    "Web-scraped text often contains HTML tags like `<br>`, `<div>`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove HTML tags from text using BeautifulSoup\"\"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "# Example\n",
    "html_text = \"This movie was <b>amazing</b>!<br><br>I loved it!\"\n",
    "clean_text = remove_html_tags(html_text)\n",
    "\n",
    "print(\"🏷️ HTML Tag Removal Example:\")\n",
    "print(f\"Before: {html_text}\")\n",
    "print(f\"After:  {clean_text}\")\n",
    "print(\"\\n✨ Transformation: Removed <b> and <br> tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Expand Contractions 📝\n",
    "\n",
    "Convert \"don't\" → \"do not\", \"it's\" → \"it is\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"Expand contractions in text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Example\n",
    "contraction_text = \"I can't believe it's not better! They won't improve.\"\n",
    "expanded_text = expand_contractions(contraction_text)\n",
    "\n",
    "print(\"📝 Contraction Expansion Example:\")\n",
    "print(f\"Before: {contraction_text}\")\n",
    "print(f\"After:  {expanded_text}\")\n",
    "print(\"\\n✨ Transformations:\")\n",
    "print(\"   • can't → cannot\")\n",
    "print(\"   • it's → it is\")\n",
    "print(\"   • won't → will not\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Convert to Lowercase 🔡\n",
    "\n",
    "Normalize text by converting everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(text):\n",
    "    \"\"\"Convert text to lowercase\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "# Example\n",
    "mixed_case = \"This MOVIE was AMAZING! I Love It!\"\n",
    "lower_text = to_lowercase(mixed_case)\n",
    "\n",
    "print(\"🔡 Lowercase Conversion Example:\")\n",
    "print(f\"Before: {mixed_case}\")\n",
    "print(f\"After:  {lower_text}\")\n",
    "print(\"\\n✨ Why this matters: 'Movie', 'MOVIE', and 'movie' are now the same!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Remove Special Characters & Punctuation 🚫\n",
    "\n",
    "Keep only alphabetic characters and spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \"\"\"Remove special characters and punctuation, keep only letters and spaces\"\"\"\n",
    "    # Remove everything except letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Example\n",
    "special_text = \"Amazing movie!!! 10/10 would watch again... #BestMovie @friends 😍\"\n",
    "clean_special = remove_special_characters(special_text)\n",
    "\n",
    "print(\"🚫 Special Character Removal Example:\")\n",
    "print(f\"Before: {special_text}\")\n",
    "print(f\"After:  {clean_special}\")\n",
    "print(\"\\n✨ Removed: !!!, 10/10, ..., #, @, 😍\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Remove Extra Spaces 🧹\n",
    "\n",
    "Clean up multiple spaces and trim text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    \"\"\"Remove extra spaces and trim text\"\"\"\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove leading and trailing spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Example\n",
    "spaced_text = \"  This   movie    was     amazing    !  \"\n",
    "clean_spaced = remove_extra_spaces(spaced_text)\n",
    "\n",
    "print(\"🧹 Extra Space Removal Example:\")\n",
    "print(f\"Before: '{spaced_text}'\")\n",
    "print(f\"After:  '{clean_spaced}'\")\n",
    "print(f\"\\n✨ Character count: {len(spaced_text)} → {len(clean_spaced)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Remove Stopwords (Optional) 🛑\n",
    "\n",
    "Remove common words that don't add meaning (the, is, at, which, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from text\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "# Example\n",
    "text_with_stopwords = \"the movie was very good and i would watch it again\"\n",
    "text_without_stopwords = remove_stopwords(text_with_stopwords)\n",
    "\n",
    "print(\"🛑 Stopword Removal Example:\")\n",
    "print(f\"Before: {text_with_stopwords}\")\n",
    "print(f\"After:  {text_without_stopwords}\")\n",
    "print(\"\\n✨ Removed words: the, was, very, and, i, would, it\")\n",
    "print(\"\\n⚠️ Note: Sometimes stopwords are important for sentiment!\")\n",
    "print(\"   Example: 'not good' → 'good' (meaning reversed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Lemmatization 🌱\n",
    "\n",
    "Reduce words to their root form (running → run, better → good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize words in text\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Example\n",
    "text_to_lemmatize = \"the movies were running beautifully\"\n",
    "lemmatized_text = lemmatize_text(text_to_lemmatize)\n",
    "\n",
    "print(\"🌱 Lemmatization Example:\")\n",
    "print(f\"Before: {text_to_lemmatize}\")\n",
    "print(f\"After:  {lemmatized_text}\")\n",
    "print(\"\\n✨ Transformations:\")\n",
    "print(\"   • movies → movie\")\n",
    "print(\"   • running → running (context needed for verb)\")\n",
    "print(\"   • beautifully → beautifully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Complete Cleaning Pipeline\n",
    "\n",
    "Now let's combine all steps into one powerful cleaning function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, \n",
    "               remove_html=True,\n",
    "               expand_contract=True,\n",
    "               lowercase=True,\n",
    "               remove_special=True,\n",
    "               remove_spaces=True,\n",
    "               remove_stop=False,\n",
    "               lemmatize=False):\n",
    "    \"\"\"\n",
    "    Complete text cleaning pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text to clean\n",
    "    remove_html : bool\n",
    "        Remove HTML tags\n",
    "    expand_contract : bool\n",
    "        Expand contractions\n",
    "    lowercase : bool\n",
    "        Convert to lowercase\n",
    "    remove_special : bool\n",
    "        Remove special characters\n",
    "    remove_spaces : bool\n",
    "        Remove extra spaces\n",
    "    remove_stop : bool\n",
    "        Remove stopwords (optional)\n",
    "    lemmatize : bool\n",
    "        Apply lemmatization (optional)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Track cleaning steps for visualization\n",
    "    steps = []\n",
    "    \n",
    "    # Step 1: Remove HTML tags\n",
    "    if remove_html:\n",
    "        text = remove_html_tags(text)\n",
    "        steps.append(\"HTML removed\")\n",
    "    \n",
    "    # Step 2: Expand contractions\n",
    "    if expand_contract:\n",
    "        text = expand_contractions(text)\n",
    "        steps.append(\"Contractions expanded\")\n",
    "    \n",
    "    # Step 3: Convert to lowercase\n",
    "    if lowercase:\n",
    "        text = to_lowercase(text)\n",
    "        steps.append(\"Lowercased\")\n",
    "    \n",
    "    # Step 4: Remove special characters\n",
    "    if remove_special:\n",
    "        text = remove_special_characters(text)\n",
    "        steps.append(\"Special chars removed\")\n",
    "    \n",
    "    # Step 5: Remove extra spaces\n",
    "    if remove_spaces:\n",
    "        text = remove_extra_spaces(text)\n",
    "        steps.append(\"Extra spaces removed\")\n",
    "    \n",
    "    # Step 6: Remove stopwords (optional)\n",
    "    if remove_stop:\n",
    "        text = remove_stopwords(text)\n",
    "        steps.append(\"Stopwords removed\")\n",
    "    \n",
    "    # Step 7: Lemmatization (optional)\n",
    "    if lemmatize:\n",
    "        text = lemmatize_text(text)\n",
    "        steps.append(\"Lemmatized\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"✅ Complete cleaning pipeline created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Testing the Complete Pipeline\n",
    "\n",
    "Let's test our pipeline on a real messy review!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messy test review\n",
    "messy_review = \"\"\"\n",
    "<b>WORST MOVIE EVER!!!</b><br><br>\n",
    "I can't believe I've wasted 2 hours on this... The acting was TERRIBLE!!! \n",
    "The plot didn't make ANY sense... #WasteOfTime @BadMovies 😤😤\n",
    "I wouldn't recommend this to anyone!!! Save your $$$ and time...\n",
    "\"\"\"\n",
    "\n",
    "# Clean with different settings\n",
    "print(\"🎬 ORIGINAL MESSY REVIEW:\")\n",
    "print(\"=\"*80)\n",
    "print(messy_review)\n",
    "\n",
    "print(\"\\n✨ BASIC CLEANING (recommended):\")\n",
    "print(\"=\"*80)\n",
    "basic_clean = clean_text(messy_review, remove_stop=False, lemmatize=False)\n",
    "print(basic_clean)\n",
    "\n",
    "print(\"\\n🔥 AGGRESSIVE CLEANING (with stopwords removal):\")\n",
    "print(\"=\"*80)\n",
    "aggressive_clean = clean_text(messy_review, remove_stop=True, lemmatize=True)\n",
    "print(aggressive_clean)\n",
    "\n",
    "print(\"\\n📊 Cleaning Statistics:\")\n",
    "print(f\"Original length: {len(messy_review)} characters\")\n",
    "print(f\"Basic clean length: {len(basic_clean)} characters\")\n",
    "print(f\"Aggressive clean length: {len(aggressive_clean)} characters\")\n",
    "print(f\"Reduction: {100*(1-len(basic_clean)/len(messy_review)):.1f}% (basic)\")\n",
    "print(f\"Reduction: {100*(1-len(aggressive_clean)/len(messy_review)):.1f}% (aggressive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Applying to IMDB Dataset\n",
    "\n",
    "Now let's clean our entire IMDB dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample for faster processing (you can increase this)\n",
    "sample_size = 5000  # Adjust based on time available\n",
    "df_sample = df.head(sample_size).copy()\n",
    "\n",
    "print(f\"📊 Processing {sample_size} reviews...\")\n",
    "print(\"⏳ This may take a minute...\\n\")\n",
    "\n",
    "# Apply cleaning\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Clean the reviews\n",
    "df_sample['cleaned_review'] = df_sample['review'].apply(\n",
    "    lambda x: clean_text(x, remove_stop=False, lemmatize=False)\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"✅ Cleaning completed in {end_time - start_time:.2f} seconds!\")\n",
    "print(f\"⚡ Average time per review: {(end_time - start_time)/sample_size:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Before vs After Comparison\n",
    "\n",
    "Let's visualize the impact of our cleaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show before and after examples\n",
    "print(\"🎬 CLEANING EXAMPLES FROM DATASET\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\n📌 Example {i+1}:\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    original = df_sample.loc[i, 'review']\n",
    "    cleaned = df_sample.loc[i, 'cleaned_review']\n",
    "    \n",
    "    # Show first 200 characters\n",
    "    print(f\"BEFORE ({len(original)} chars):\")\n",
    "    print(f\"{original[:200]}...\" if len(original) > 200 else original)\n",
    "    \n",
    "    print(f\"\\nAFTER ({len(cleaned)} chars):\")\n",
    "    print(f\"{cleaned[:200]}...\" if len(cleaned) > 200 else cleaned)\n",
    "    \n",
    "    print(f\"\\n✨ Reduction: {100*(1-len(cleaned)/len(original)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "df_sample['original_length'] = df_sample['review'].str.len()\n",
    "df_sample['cleaned_length'] = df_sample['cleaned_review'].str.len()\n",
    "df_sample['reduction_pct'] = 100 * (1 - df_sample['cleaned_length'] / df_sample['original_length'])\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Length distribution\n",
    "axes[0].hist(df_sample['original_length'], bins=50, alpha=0.5, label='Original', color='red')\n",
    "axes[0].hist(df_sample['cleaned_length'], bins=50, alpha=0.5, label='Cleaned', color='green')\n",
    "axes[0].set_xlabel('Character Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('📊 Review Length Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Reduction percentage\n",
    "axes[1].hist(df_sample['reduction_pct'], bins=30, color='blue', alpha=0.7)\n",
    "axes[1].set_xlabel('Reduction (%)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('✂️ Character Reduction Distribution')\n",
    "axes[1].axvline(df_sample['reduction_pct'].mean(), color='red', linestyle='--', label=f'Mean: {df_sample[\"reduction_pct\"].mean():.1f}%')\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot 3: Before vs After scatter\n",
    "axes[2].scatter(df_sample['original_length'], df_sample['cleaned_length'], alpha=0.3, s=1)\n",
    "axes[2].plot([0, df_sample['original_length'].max()], [0, df_sample['original_length'].max()], 'r--', alpha=0.5)\n",
    "axes[2].set_xlabel('Original Length')\n",
    "axes[2].set_ylabel('Cleaned Length')\n",
    "axes[2].set_title('🔄 Original vs Cleaned Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📈 Cleaning Statistics:\")\n",
    "print(f\"Average reduction: {df_sample['reduction_pct'].mean():.1f}%\")\n",
    "print(f\"Median reduction: {df_sample['reduction_pct'].median():.1f}%\")\n",
    "print(f\"Total characters removed: {(df_sample['original_length'].sum() - df_sample['cleaned_length'].sum()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌟 Word Cloud Visualization\n",
    "\n",
    "Let's create beautiful word clouds to see the most common words before and after cleaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate positive and negative reviews\n",
    "positive_reviews = df_sample[df_sample['sentiment'] == 'positive']['cleaned_review'].str.cat(sep=' ')\n",
    "negative_reviews = df_sample[df_sample['sentiment'] == 'negative']['cleaned_review'].str.cat(sep=' ')\n",
    "\n",
    "# Create word clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Positive reviews word cloud\n",
    "wordcloud_pos = WordCloud(width=800, height=400, \n",
    "                          background_color='white',\n",
    "                          colormap='Greens',\n",
    "                          max_words=100).generate(positive_reviews)\n",
    "axes[0].imshow(wordcloud_pos, interpolation='bilinear')\n",
    "axes[0].set_title('😊 Positive Reviews - Most Common Words', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Negative reviews word cloud\n",
    "wordcloud_neg = WordCloud(width=800, height=400,\n",
    "                          background_color='white',\n",
    "                          colormap='Reds',\n",
    "                          max_words=100).generate(negative_reviews)\n",
    "axes[1].imshow(wordcloud_neg, interpolation='bilinear')\n",
    "axes[1].set_title('😞 Negative Reviews - Most Common Words', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 Insights:\")\n",
    "print(\"- Positive reviews often mention: 'great', 'good', 'love', 'best'\")\n",
    "print(\"- Negative reviews often mention: 'bad', 'worst', 'boring', 'waste'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Top Words Analysis\n",
    "\n",
    "Let's find the most common words in positive vs negative reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get top words for positive reviews\n",
    "positive_words = ' '.join(df_sample[df_sample['sentiment'] == 'positive']['cleaned_review']).split()\n",
    "positive_counter = Counter(positive_words)\n",
    "top_positive = positive_counter.most_common(15)\n",
    "\n",
    "# Get top words for negative reviews\n",
    "negative_words = ' '.join(df_sample[df_sample['sentiment'] == 'negative']['cleaned_review']).split()\n",
    "negative_counter = Counter(negative_words)\n",
    "top_negative = negative_counter.most_common(15)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Positive words bar chart\n",
    "words_pos, counts_pos = zip(*top_positive)\n",
    "axes[0].barh(words_pos, counts_pos, color='green', alpha=0.7)\n",
    "axes[0].set_xlabel('Frequency')\n",
    "axes[0].set_title('😊 Top 15 Words in Positive Reviews')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Negative words bar chart\n",
    "words_neg, counts_neg = zip(*top_negative)\n",
    "axes[1].barh(words_neg, counts_neg, color='red', alpha=0.7)\n",
    "axes[1].set_xlabel('Frequency')\n",
    "axes[1].set_title('😞 Top 15 Words in Negative Reviews')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 Key Observations:\")\n",
    "print(\"- Common words appear in both positive and negative reviews ('movie', 'film')\")\n",
    "print(\"- Sentiment-specific words are clearly different ('great' vs 'bad')\")\n",
    "print(\"- This shows why cleaning helps models focus on meaningful differences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Saving Cleaned Data\n",
    "\n",
    "Let's save our cleaned dataset for future use (like fine-tuning)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "output_file = '/content/IMDB_cleaned.csv'\n",
    "df_sample[['review', 'cleaned_review', 'sentiment']].to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✅ Cleaned data saved to: {output_file}\")\n",
    "print(f\"📊 File contains {len(df_sample)} cleaned reviews\")\n",
    "print(f\"📁 File size: {df_sample.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Show sample of saved data\n",
    "print(\"\\n📄 Sample of saved data:\")\n",
    "df_sample[['cleaned_review', 'sentiment']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Quick Quality Check\n",
    "\n",
    "Let's verify our cleaning worked correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality checks\n",
    "print(\"🔍 Data Quality Checks:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for empty reviews\n",
    "empty_reviews = df_sample[df_sample['cleaned_review'].str.strip() == ''].shape[0]\n",
    "print(f\"✓ Empty reviews after cleaning: {empty_reviews}\")\n",
    "\n",
    "# Check for HTML tags\n",
    "html_remaining = df_sample['cleaned_review'].str.contains('<|>', regex=True).sum()\n",
    "print(f\"✓ Reviews with HTML tags remaining: {html_remaining}\")\n",
    "\n",
    "# Check for special characters\n",
    "special_remaining = df_sample['cleaned_review'].str.contains('[^a-zA-Z\\s]', regex=True).sum()\n",
    "print(f\"✓ Reviews with special characters: {special_remaining}\")\n",
    "\n",
    "# Check for numbers\n",
    "numbers_remaining = df_sample['cleaned_review'].str.contains('\\d', regex=True).sum()\n",
    "print(f\"✓ Reviews with numbers: {numbers_remaining}\")\n",
    "\n",
    "print(\"\\n✅ All quality checks passed!\" if all([empty_reviews == 0, html_remaining == 0, \n",
    "                                                  special_remaining == 0, numbers_remaining == 0]) \n",
    "      else \"⚠️ Some issues detected - review cleaning parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Key Takeaways\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "1. **📊 Data Cleaning is Essential**\n",
    "   - Raw text contains noise that confuses models\n",
    "   - Cleaning can reduce text size by 20-30% while preserving meaning\n",
    "\n",
    "2. **🔧 Core Cleaning Steps**\n",
    "   - Remove HTML tags and special characters\n",
    "   - Normalize text (lowercase, contractions)\n",
    "   - Clean formatting (spaces, punctuation)\n",
    "\n",
    "3. **⚖️ Balance is Key**\n",
    "   - Don't over-clean (removing stopwords can hurt sentiment analysis)\n",
    "   - Keep domain-specific requirements in mind\n",
    "\n",
    "4. **📈 Visual Impact**\n",
    "   - Word clouds show clear differences after cleaning\n",
    "   - Cleaned data reveals true patterns in text\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "- Use this cleaned data for model fine-tuning\n",
    "- Experiment with different cleaning parameters\n",
    "- Apply these techniques to your domain-specific data\n",
    "\n",
    "### 💡 Remember:\n",
    "**\"Garbage In, Garbage Out\" - Clean data leads to better models!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Bonus: Custom Cleaning Function for Your Domain\n",
    "\n",
    "Here's a template you can adapt for electrical engineering data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_technical_text(text):\n",
    "    \"\"\"\n",
    "    Custom cleaning for technical/electrical engineering text\n",
    "    \n",
    "    This is an example - adapt for your specific needs!\n",
    "    \"\"\"\n",
    "    # Keep technical terms and units\n",
    "    # Example: preserve \"220V\", \"50Hz\", \"AC/DC\"\n",
    "    \n",
    "    # Normalize units\n",
    "    text = re.sub(r'(\\d+)\\s*[Vv]olts?', r'\\1V', text)\n",
    "    text = re.sub(r'(\\d+)\\s*[Aa]mps?', r'\\1A', text)\n",
    "    text = re.sub(r'(\\d+)\\s*[Hh]ertz', r'\\1Hz', text)\n",
    "    \n",
    "    # Keep important technical abbreviations\n",
    "    # AC, DC, PCB, MCU, etc.\n",
    "    \n",
    "    # Standard cleaning\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example\n",
    "technical_text = \"The transformer operates at 220 Volts AC with 50 hertz frequency.\"\n",
    "cleaned_technical = clean_technical_text(technical_text)\n",
    "print(f\"Technical text cleaning:\")\n",
    "print(f\"Before: {technical_text}\")\n",
    "print(f\"After:  {cleaned_technical}\")\n",
    "print(\"\\n💡 Tip: Customize this function for your specific domain!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Resources & References\n",
    "\n",
    "### Useful Libraries:\n",
    "- **NLTK**: Natural Language Toolkit for text processing\n",
    "- **spaCy**: Industrial-strength NLP (alternative to NLTK)\n",
    "- **TextBlob**: Simplified text processing\n",
    "\n",
    "### Further Learning:\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [Text Preprocessing Guide](https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html)\n",
    "- [Regex Tutorial](https://regexone.com/)\n",
    "\n",
    "### For Electrical Engineers:\n",
    "- Apply these techniques to:\n",
    "  - Equipment maintenance logs\n",
    "  - Fault reports\n",
    "  - Technical documentation\n",
    "  - Sensor data descriptions\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 Congratulations!\n",
    "\n",
    "You've successfully learned and implemented text data cleaning! Your data is now ready for:\n",
    "- 🤖 Model training\n",
    "- 🎯 Fine-tuning\n",
    "- 📊 Analysis\n",
    "- 🚀 Deployment\n",
    "\n",
    "**Happy Cleaning! 🧹✨**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}